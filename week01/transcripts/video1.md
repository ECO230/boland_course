
# Transcript — Video 1 (ETL)

It's usually one of our first task is figuring out how to get the data where we need it to be in order to use it and do our analysis on it. So generally when we're getting data, it's going to follow this type of a process, extract, transform load or ETL process. A lot of the data that we're gonna be working with, especially for this course, is going to be data that other people have gathered what's known as secondary data. Later on in the class, we will talk little bit about primary data, how to, how to gather data for a specific purpose. But the vast majority of the time we're gonna be using data that's already existing and whether it is from a third party source or internally within our company has sales database or something like that. We usually don't have a lot of say in how their data is structured, how it's set up. So we need to figure out how to get that data in the form that it's in currently. And then get it into a form that we're gonna be able to do. Our nails the sun.

The first step in that process is to extract the data. And that is to get the data from wherever it is to our computer, to where we're gonna be working with it at. That data can take on a number of different forms. Sometimes the data is relatively clean. If it is a something like this where we have a structured database where everything is set up just so you know, each point of data is in its own row, each specific variables in its own column. So sometimes it might be pretty much in the form that we need to use it, right where it is.

Other times you might be grabbing it from a website or something like that. If you wanted to do some analysis looking at census data, you will go to the Census Bureau's website and download data from there. It might just be unstructured data just pouring into, into a database, some sort of a sensor or something that is tracking data, looking at a motion sensor or something like that. Where, you know, every time that that sensor is triggered, it it changes from a truth or false yes or no sort of a thing. Measuring the brightness or measuring the voltage of a certain sensor. Measuring whether or not motion was detected on a video camera. That data might just be constantly pouring in. And then we have to figure out, okay, how we're gonna deal with this L. We're going to figure out what the timestamp was. What does this data, what is it telling us?

That first step is to is to extract that data. So get it out of where it is right now. Get it to where we need it to be. Once we have that data downloaded or transferred over to where we're gonna be using it or connected to, you know, sometimes we don't actually need the data, we just make a connection to where it's at. Next step is to transform that data to get it into a format that's going to be convenient for us to use that we can do analysis on without a constantly having to go in and edit things and change things and recalculate. So that's the T part of the ETL process transform to get the data from the form that it's in when we get it, to change it to the form that we're gonna be using it for our analysis.

So a lot of times we're editing this data, so it's gonna be stored in a consistent way. It's gonna be efficient for us to go and look at it. There's certain rules that we're going to follow that's gonna make our lives much easier when it comes time to analyze that data or build a chart or do some analysis on it. There are some standard ways to keep that where any tool that we're going to connect to that data width is going to be able to look at it in a similar way.

The example on the top here, that a good before if we download data from the Census Bureau, It's gonna look something like this. So this data is stored in a way where we have gigabytes of data on. However many million people live in the country. All that data is stored on a server somewhere in a way that's going to be convenient for the Census Bureau. That's not necessarily convenient for us. You can see we have a lot of empty space here. If category one, category to category three, you might not know what those things mean.

A lot of times we need to transform that data to get it into something. It's going to be a little bit more convenient for us to use consistent way to measure what was the time that his data was recorded? Consistent way to measure geographically, what was their zip code? What was the date of what was what were the different categories that it falls into? A lot of times we can store that in a much more efficient way. We do it as part of the transform process.

Last step then is to load it into the program that we're gonna be using to do our analysis. In this can be anything. Sometimes we go from a database, we transform it and put it right back into that same database. This doesn't necessarily have to be different programs that we're using.

(… transcript continues …)
