{
  "hash": "f04e82b65080379e8c6abba25b5af7d7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ECO 230 – Week 1 Lecture\"\nauthor: \"Mike Boland, MBA\"\nformat:\n  revealjs:\n    theme: simple\n    css: ../shared/styles/accessibility.css\n    slide-number: true\n    hash: true\n    controls: true\n    transition: fade\n    pdf-export: true\n    toc: false\n    self-contained: false\nexecute:\n  echo: false\n---\n\n```{=html}\n<!--\nThis Quarto deck was generated from Week 1 PPT + three transcripts.\nHidden titles are provided using the .sr-only class (defined in accessibility.css).\nReplace any media/* placeholder image paths with your actual files.\nIf videos are hosted in Canvas, leave the external links as-is and keep the transcripts here.\n-->\n```\n\n## [Course & Section]{.sr-only}\n\n**ECO 230**\\\n**Section 005**\n\n------------------------------------------------------------------------\n\n## [Instructor & Contact]{.sr-only}\n\n**Mike Boland, MBA**\\\n**mboland\\@uwlax.edu**\\\n**Phone:** 608-385-6497\n\n------------------------------------------------------------------------\n\n## [What is Statistics?]{.sr-only}\n\n**What is statistics?**\n\n![](media/images/w01_s03_image.jpeg){fig-alt=\"Photo of several dogs lying on a blanket; playful opener for the topic.\" width=\"70%\"}\n\n------------------------------------------------------------------------\n\n## [Analytics Definition]{.sr-only}\n\n**Analytics is the art of finding out what is in your data.**\n\n![](media/images/decorative-blanket.jpg){fig-alt=\"\" aria-hidden=\"true\" width=\"65%\"}\n\n------------------------------------------------------------------------\n\n## [Analytics vs. Certainty]{.sr-only}\n\n**If you can (with some effort) find the answer with certainty you are using analytics.**\n\n![](media/images/decorative-blanket-2.jpg){fig-alt=\"\" aria-hidden=\"true\" width=\"65%\"}\n\n------------------------------------------------------------------------\n\n## [Analytics vs. Statistics (1)]{.sr-only}\n\n**Analytics deals with what you know.**\\\n**Statistics deals with what you do not know.**\n\n![](media/images/decorative-blanket-3.jpg){fig-alt=\"\" aria-hidden=\"true\" width=\"65%\"}\n\n------------------------------------------------------------------------\n\n## [Analytics vs. Statistics (2)]{.sr-only}\n\n**Analytics helps you get good questions.**\\\n**Statistics helps you get good answers.**\n\n![](media/images/decorative-blanket-4.jpg){fig-alt=\"\" aria-hidden=\"true\" width=\"65%\"}\n\n------------------------------------------------------------------------\n\n## [Statistics as Decision-Making]{.sr-only}\n\n**Statistics is the science of making decisions under uncertainty.**\n\n![](media/images/decorative-blanket-5.jpg){fig-alt=\"\" aria-hidden=\"true\" width=\"65%\"}\n\n------------------------------------------------------------------------\n\n## [Learning Curve]{.sr-only}\n\n![](media/images/learning-curve.png){fig-alt=\"Stylized learning curve chart with Time on the x-axis and Skill on the y-axis, labeled phases ‘This Sucks’, ‘This $ucks’, and ‘This Is Fun’.\" width=\"75%\"}\n\n**Learning Curve**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week_01_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n::: center\nstep1_blue.png\n\nstep2_blue_orange.png\n\nstep3_all.png\n:::\n\n------------------------------------------------------------------------\n\n## [I’m Old (humor)]{.sr-only}\n\n![](media/images/im-old.png){fig-alt=\"Lightweight humorous image labeled ‘I’m Old’.\" width=\"60%\"}\n\n------------------------------------------------------------------------\n\n## [ETL Overview]{.sr-only}\n\n**Extract – Transform – Load**\n\n-   **Extract:** get data from its source to your working environment\\\n-   **Transform:** clean, standardize, reshape to a consistent structure\\\n-   **Load:** place the prepared data into the analysis tool (Excel, R, Tableau, DB)\n\n![](media/images/etl-icons.png){fig-alt=\"Icon trio depicting Extract, Transform, and Load steps.\" width=\"70%\"}\n\n------------------------------------------------------------------------\n\n## [Transform Emphasis]{.sr-only}\n\n**Extract – *Transform* – Load**\\\nEdit data so it is stored in a **consistent** and **efficient** way.\n\n![](media/images/transform-focus.png){fig-alt=\"Diagram emphasizing the Transform phase: cleaning, reshaping, standardizing fields like dates, categories, and locations.\" width=\"70%\"}\n\n------------------------------------------------------------------------\n\n## [Load Destinations]{.sr-only}\n\n**Extract – Transform – *Load***\n\n**Destinations:** Databases, Excel, R, Tableau, SPSS\n\n![](media/images/load-destinations.png){fig-alt=\"Row of logos/icons for common analysis tools: database cylinder, Excel, R, Tableau, SPSS.\" width=\"80%\"}\n\n------------------------------------------------------------------------\n\n## [Video: ETL (watch on Canvas) + Transcript]{.sr-only}\n\n> **Watch on Canvas:** *ETL overview video*\\\n> (Replace `#` with your Canvas URL.)\n\n[▶️ Open video in Canvas](#)\n\n<details>\n\n<summary><strong>Transcript — ETL</strong> (click to expand)</summary>\n\n\n# Transcript — Video 1 (ETL)\n\nIt's usually one of our first task is figuring out how to get the data where we need it to be in order to use it and do our analysis on it. So generally when we're getting data, it's going to follow this type of a process, extract, transform load or ETL process. A lot of the data that we're gonna be working with, especially for this course, is going to be data that other people have gathered what's known as secondary data. Later on in the class, we will talk little bit about primary data, how to, how to gather data for a specific purpose. But the vast majority of the time we're gonna be using data that's already existing and whether it is from a third party source or internally within our company has sales database or something like that. We usually don't have a lot of say in how their data is structured, how it's set up. So we need to figure out how to get that data in the form that it's in currently. And then get it into a form that we're gonna be able to do. Our nails the sun.\n\nThe first step in that process is to extract the data. And that is to get the data from wherever it is to our computer, to where we're gonna be working with it at. That data can take on a number of different forms. Sometimes the data is relatively clean. If it is a something like this where we have a structured database where everything is set up just so you know, each point of data is in its own row, each specific variables in its own column. So sometimes it might be pretty much in the form that we need to use it, right where it is.\n\nOther times you might be grabbing it from a website or something like that. If you wanted to do some analysis looking at census data, you will go to the Census Bureau's website and download data from there. It might just be unstructured data just pouring into, into a database, some sort of a sensor or something that is tracking data, looking at a motion sensor or something like that. Where, you know, every time that that sensor is triggered, it it changes from a truth or false yes or no sort of a thing. Measuring the brightness or measuring the voltage of a certain sensor. Measuring whether or not motion was detected on a video camera. That data might just be constantly pouring in. And then we have to figure out, okay, how we're gonna deal with this L. We're going to figure out what the timestamp was. What does this data, what is it telling us?\n\nThat first step is to is to extract that data. So get it out of where it is right now. Get it to where we need it to be. Once we have that data downloaded or transferred over to where we're gonna be using it or connected to, you know, sometimes we don't actually need the data, we just make a connection to where it's at. Next step is to transform that data to get it into a format that's going to be convenient for us to use that we can do analysis on without a constantly having to go in and edit things and change things and recalculate. So that's the T part of the ETL process transform to get the data from the form that it's in when we get it, to change it to the form that we're gonna be using it for our analysis.\n\nSo a lot of times we're editing this data, so it's gonna be stored in a consistent way. It's gonna be efficient for us to go and look at it. There's certain rules that we're going to follow that's gonna make our lives much easier when it comes time to analyze that data or build a chart or do some analysis on it. There are some standard ways to keep that where any tool that we're going to connect to that data width is going to be able to look at it in a similar way.\n\nThe example on the top here, that a good before if we download data from the Census Bureau, It's gonna look something like this. So this data is stored in a way where we have gigabytes of data on. However many million people live in the country. All that data is stored on a server somewhere in a way that's going to be convenient for the Census Bureau. That's not necessarily convenient for us. You can see we have a lot of empty space here. If category one, category to category three, you might not know what those things mean.\n\nA lot of times we need to transform that data to get it into something. It's going to be a little bit more convenient for us to use consistent way to measure what was the time that his data was recorded? Consistent way to measure geographically, what was their zip code? What was the date of what was what were the different categories that it falls into? A lot of times we can store that in a much more efficient way. We do it as part of the transform process.\n\nLast step then is to load it into the program that we're gonna be using to do our analysis. In this can be anything. Sometimes we go from a database, we transform it and put it right back into that same database. This doesn't necessarily have to be different programs that we're using.\n\n(… transcript continues …)\n\n\n</details>\n\n------------------------------------------------------------------------\n\n## [Unit of Analysis]{.sr-only}\n\n**Unit of Analysis**\\\nWhat does each **row / observation / grain** in this dataset represent?\n\n-   Everyone in a ZIP code\\\n-   Individual consumers\\\n-   Individual purchases\\\n-   GDP of an economy\\\n-   Population of a planet\n\n![](media/images/unit-of-analysis.png){fig-alt=\"Visual cue reminding that each dataset row should map to a clearly defined observational unit.\" width=\"70%\"}\n\n------------------------------------------------------------------------\n\n## [Data Grouping: Cross-Sectional]{.sr-only}\n\n**Data Grouping: Cross‑Sectional**\\\nA snapshot of different subjects at **one point in time**.\n\n![](media/images/cross-sectional.png){fig-alt=\"Photo of a water droplet frozen in time; metaphor for a single-time snapshot of data.\" width=\"70%\"}\n\n------------------------------------------------------------------------\n\n## [Data Grouping: Time-Series]{.sr-only}\n\n**Data Grouping: Time‑Series**\\\nTrack measurements of (often different) subjects **over time**.\n\n![](media/images/time-series.png){fig-alt=\"Illustration of growth over months; metaphor for changes tracked over time.\" width=\"70%\"}\n\n------------------------------------------------------------------------\n\n## [Data Grouping: Panel]{.sr-only}\n\n**Data Grouping: Panel**\\\nTrack the **same subjects** repeatedly over time.\n\n![](media/images/panel-data.png){fig-alt=\"Illustration of multiple parallel timelines for the same subjects; metaphor for panel data.\" width=\"70%\"}\n\n------------------------------------------------------------------------\n\n## [Tall vs Wide; Know Your Structure]{.sr-only}\n\n**Understand the Structure of your data**\n\n-   **Tall** — more rows, fewer columns\\\n-   **Wide** — more columns, fewer rows\n\n**Example (Tall):**\n\n| Name | Variable | Value |\n|------|----------|-------|\n| Mike | Gender   | Male  |\n| Mike | Age      | 43    |\n| Mike | Monday   | 6.5   |\n| Mike | Tuesday  | 7.2   |\n\n**Example (Wide):**\n\n| Name | Gender | Age | Monday | Tuesday |\n|------|--------|-----|--------|---------|\n| Mike | Male   | 43  | 6.5    | 7.2     |\n\n------------------------------------------------------------------------\n\n## [Rows vs Columns]{.sr-only}\n\n**Rows** stack **observations**.\\\n**Columns** hold **variables**.\n\n![](media/images/rows-columns.png){fig-alt=\"Simple grid with arrows: rows for observations (left-to-right), columns for variables (top-to-bottom).\" width=\"70%\"}\n\n------------------------------------------------------------------------\n\n## [Data Types Overview]{.sr-only}\n\n**Categories** • **Numbers**\\\n**Continuous** • **Discrete**\\\n**Categorical** (**logical/true–false**, **ordinal**, **factor**)\\\n**Numeric** (**integer**, **double/real**)\n\n------------------------------------------------------------------------\n\n## [Observations — Atomic Data Example]{.sr-only}\n\n**Observations (Atomic Data)** — sleep tracking example\n\n-   Start Time: 10:53 PM\\\n-   End Time: 6:18 AM\\\n-   REM: 87 minutes; Deep: 231; Light: 127\\\n-   Total Sleep: 445; Quality: 92%\\\n-   Name: Mike; Date: 9/7/2023; Gender: Male; Location: Bed\n\n![](media/images/atomic-data.png){fig-alt=\"Cube metaphor for an atomic observation; each row represents one measured event with associated variables.\" width=\"60%\"}\n\n------------------------------------------------------------------------\n\n## [Dimensions & Measures]{.sr-only}\n\n**Dimensions** *(categories, filters, axis labels)*: Month, Location, Person\\\n**Measures** *(aggregations)*: Averages, Standard Deviations, Counts, Sums\n\n**Example (June, Glamping):**\\\nMean = 523.12 minutes · Median = 600 minutes · Quality = 82.35% · N = 2,345\n\n![](media/images/dimensions-measures.png){fig-alt=\"Schematic showing dimensions (Month, Location, Person) and measures (Average, STD, Count).\" width=\"70%\"}\n\n------------------------------------------------------------------------\n\n## [Example Table]{.sr-only}\n\n**Measures vs. Dimensions; Tall vs. Wide**\n\n\\``{.markdown} Name,Gender,Location,Month,Day,Start,End,REM,Deep,Light,Total Cam,M,Glamping,Jan,Mon,10:17 PM,8:21 AM,10,80,28,118 Cam,M,Home,Jul,Mon,8:20 PM,9:32 AM,100,179,234,513 John,M,Home,Aug,Mon,8:32 PM,11:32 AM,86,359,233,677 Lydia,F,Home,Aug,Mon,8:34 AM,4:45 AM,98,299,219,616`\n",
    "supporting": [
      "week_01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}